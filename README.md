# Building an AWS Data Warehouse for the Sparkify Music App

## Challenge

<p align=justify>A startup called Sparkify has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.</p>

<p align=justify>They'd like a data engineer to build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.</p>

## Project Description

### Data

<p align=justify>You'll be working with two datasets that reside in S3. Here are the S3 links for each:</p>

• Song data: ``s3://udacity-dend/song_data``

• Log data: ``s3://udacity-dend/log_data``

Log data json path: ``s3://udacity-dend/log_json_path.json``

<p align=justify>The first dataset is a subset of real data from the <a href=https://labrosa.ee.columbia.edu/millionsong/>Million Song Dataset</a>. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.</p>

<p align=justify>The second dataset consists of log files in JSON format generated by this <a href=https://github.com/Interana/eventsim>event simulator</a> based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset you'll be working with are partitioned by year and month. </p>

## Code Description

### ``dwh.cfg``

<p align=justify>Configuration file containing a series of default values to create the necessary Redshift cluster and IAM roles. The AWS key and secret key entries are empty to fill in by the user. NOTE: Never share that information publicly.</p>

### ``sql_queries.py``

<p align=justify>The script specifies the SQL commands to DROP and CREATE the staging_events and staging_songs tables to transfer the data from the source S3 to our cluster, as well as the songplays, users, songs, artists and time tables in the relational database. Additionally, it contains the COPY and INSERT commands to fill in the data and a series of SELECT utility commands to obtain information on the data to optimize the warehouse structure with distribution keys and sort keys.</p>

### ``create_cluster.py``

<p align=justify>The script defines the functions to create an Amazon Redshift Cluster to serve as the data warehouse and an IAM Role with Read Only Access policy to the cluster.</p>

### ``create_tables.py``

<p align=justify>The script defines the functions to create the Sparkify relational database, create and drop the seven tables aforementioned from the imported SQL commands in the SQL queries script.</p>

### ``etl.py``

<p align=justify>The script extracts, transforms and loads the data into the required intermediate staging tables from the song and log files in JSON format and from there into the Sparkify database.</p>

## ``optimize.py``

<p align=justify>The script returns information on the data in the fact and dimension tables to solve issues with data skewness and increase efficiency in running JOIN queries by database design. Accordingly, start_time was chosen as the distribution key in the songplays and time tables. All its data was distinct, already contained a non-negligible number of entries and is likely to contain the largest amount of unique values (if we assume the app will be successful and could be operative indefenitely, but artists, songs and users ids will be limited and skew the data). songplay_id, user_id, song_id, artist_id and start_time were respectively chosen as sort keys for the songplay, users, songs, artists and time tables.</p>

## Running the Code

### 1. Fill in the configuration file with your AWS access key and secret key.

### 2. Run the following scripts on the command line.

    $python3 create_cluster.py
    $python3 create_tables.py
    $python3 etl.py
    
### Optional: visualize the optimization info running the followinf script on the command line.

    $python3 optimize.py
